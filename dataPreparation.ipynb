{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step to funbction mapping\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import javalang\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureStepMatcher:\n",
    "    def __init__(self, feature_files_path: str):\n",
    "        self.feature_files_path = Path(feature_files_path)\n",
    "        self.feature_steps: List[dict] = []\n",
    "        self.step_implementations: Dict[str, Dict] = {}\n",
    "        \n",
    "\n",
    "    def extract_steps_from_features(self) -> None:\n",
    "        \"\"\"Extract all steps from feature files.\"\"\"\n",
    "        feature_files = glob.glob(str(self.feature_files_path / \"*.feature\"))\n",
    "        \n",
    "        if not feature_files:\n",
    "            logger.warning(f\"No feature files found in {self.feature_files_path}\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Found {len(feature_files)} feature files\")\n",
    "        \n",
    "        for file_path in feature_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    logger.info(f\"Processing feature file: {Path(file_path).name}\")\n",
    "                    for line in file:\n",
    "                        line = line.strip()\n",
    "                        if line.startswith(('Given ', 'When ', 'Then ', 'And ')):\n",
    "                            step_type = line.split()[0]\n",
    "                            step = re.sub(f'^{step_type}\\\\s+', '', line)\n",
    "                            \n",
    "                            self.feature_steps.append({\n",
    "                                'original': step,\n",
    "                                'normalized': self.normalize_step(step),\n",
    "                                'type': step_type,\n",
    "                                'file': str(Path(file_path).name)\n",
    "                            })\n",
    "\n",
    "                            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing feature file {file_path}: {str(e)}\")\n",
    "\n",
    "    def parse_step_definitions(self, java_file_content: str, file_path: str) -> None:\n",
    "        \"\"\"Parse Java file to extract step definitions with their annotations.\"\"\"\n",
    "        try:\n",
    "            tree = javalang.parse.parse(java_file_content)\n",
    "            \n",
    "            for path, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "                for annotation in node.annotations:\n",
    "                    annotation_name = annotation.name\n",
    "                    \n",
    "                    if annotation_name in ['Given', 'When', 'Then', 'And']:\n",
    "                        try:\n",
    "                            raw_pattern = annotation.element.value.strip('\"')\n",
    "                            processed_pattern = re.sub(r'\\.+', '.star', raw_pattern)\n",
    "                            processed_pattern = re.sub(r'\\([^)]+\\)', 'VARIABLE', processed_pattern)\n",
    "                            processed_pattern = re.sub(r'\\{[^}]+\\}', 'VARIABLE', processed_pattern)\n",
    "                            processed_pattern = re.sub(r'\\?', '', processed_pattern)\n",
    "                            processed_pattern = re.sub(r'\\[[^\\]]+\\]', 'CHAR_CLASS', processed_pattern)\n",
    "                            processed_pattern = processed_pattern.replace('^', '').replace('$', '')\n",
    "                            \n",
    "                            method_lines = java_file_content.split('\\n')\n",
    "                            annotation_line = max(0, node.position.line - 2)\n",
    "                            \n",
    "                            \n",
    "                            end_line = node.position.line\n",
    "                            brace_count = 0\n",
    "                            found_start = False\n",
    "                            \n",
    "                            for i, line in enumerate(method_lines[node.position.line - 1:], node.position.line):\n",
    "                                if ('{' in line) and (not found_start):\n",
    "                                    found_start = True\n",
    "                                if found_start:\n",
    "                                    brace_count = brace_count + line.count('{') - line.count('}')\n",
    "                                    if brace_count == 0:\n",
    "                                        end_line = i + 1\n",
    "                                        break\n",
    "                            \n",
    "                            method_content = '\\n'.join(method_lines[annotation_line:end_line])\n",
    "                            \n",
    "                            normalized_pattern = self.normalize_step(processed_pattern)\n",
    "                            \n",
    "                            self.step_implementations[normalized_pattern] = {\n",
    "                                'original_pattern': raw_pattern,\n",
    "                                'processed_pattern': processed_pattern,\n",
    "                                'annotation': f'@{annotation_name}(\"{raw_pattern}\")',\n",
    "                                'method_name': node.name,\n",
    "                                'method_content': method_content,\n",
    "                                'file_path': file_path,\n",
    "                                'type': annotation_name,\n",
    "                                'parameters': self.extract_parameters(raw_pattern)\n",
    "                            }\n",
    "                            \n",
    "                            \n",
    "                        except AttributeError as e:\n",
    "                            logger.warning(f\"Error processing annotation in {file_path}: {str(e)}\")\n",
    "                            continue\n",
    "                            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing Java file {file_path}: {str(e)}\")\n",
    "\n",
    "    def extract_parameters(self, pattern: str) -> List[str]:\n",
    "        \"\"\"Extract parameter patterns from the step definition.\"\"\"\n",
    "        parameters = []\n",
    "        \n",
    "        regex_params = re.finditer(r'\\((.*?)\\)', pattern)\n",
    "        for match in regex_params:\n",
    "            param_pattern = match.group(1)\n",
    "            if param_pattern != \".*\":\n",
    "                parameters.append(param_pattern)\n",
    "        \n",
    "        cucumber_params = re.finditer(r'\\{([^}]+)\\}', pattern)\n",
    "        for match in cucumber_params:\n",
    "            parameters.append(match.group(1))\n",
    "        \n",
    "        return parameters\n",
    "\n",
    "    def normalize_step(self, step: str) -> str:\n",
    "        \"\"\"Normalize step by replacing variables and placeholders with generic tokens.\"\"\"\n",
    "        step = re.sub(r'\"[^\"]*\"', 'QUOTED_STRING', step)\n",
    "        step = re.sub(r'\\b\\d+\\b', 'NUMBER', step)\n",
    "        step = re.sub(r'\\.+', 'WILDCARD', step)\n",
    "        step = re.sub(r'\\([^)]+\\)', 'VARIABLE', step)\n",
    "        step = re.sub(r'\\{[^}]+\\}', 'VARIABLE', step)\n",
    "        step = re.sub(r'\\.[a-zA-Z]+\\b', 'FILE_EXT', step)\n",
    "        step = re.sub(r'\\[[^\\]]+\\]', 'CHAR_CLASS', step)\n",
    "        step = re.sub(r'[\\^\\$\\*\\+\\?\\[\\]\\{\\}\\|\\(\\)]', '', step)\n",
    "        return step.lower().strip()\n",
    "\n",
    "    def find_best_match(self, step: dict) -> Optional[dict]:\n",
    "        \"\"\"Find the best matching step definition using cosine similarity.\"\"\"\n",
    "        if not self.step_implementations:\n",
    "            return None\n",
    "            \n",
    "        step_text = step['normalized']\n",
    "        implementation_texts = [(pattern, impl) for pattern, impl in self.step_implementations.items()]\n",
    "        \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "            texts = [step_text] + [pattern for pattern, _ in implementation_texts]\n",
    "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "            \n",
    "            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "            \n",
    "            best_match_index = np.argmax(similarities)\n",
    "            best_match_score = similarities[best_match_index]\n",
    "            \n",
    "            if best_match_score > 0.1:\n",
    "                return {\n",
    "                    'implementation': implementation_texts[best_match_index][1],\n",
    "                    'similarity_score': best_match_score\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating similarity: {str(e)}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def match_steps_with_implementations(self) -> Dict:\n",
    "        \"\"\"Match feature steps with their implementations using cosine similarity.\"\"\"\n",
    "        matches = {}\n",
    "        \n",
    "        for step in self.feature_steps:\n",
    "            match_result = self.find_best_match(step)\n",
    "            \n",
    "            matches[step['original']] = {\n",
    "                'implementation': match_result['implementation'] if match_result else None,\n",
    "                'similarity_score': match_result['similarity_score'] if match_result else 0,\n",
    "                'type': step['type'],\n",
    "                'feature_file': step['file']\n",
    "            }\n",
    "            \n",
    "        return matches\n",
    "\n",
    "def main(suite_name):\n",
    "    feature_path = f\"/Users/ritusaini/Documents/acp-e2e-testing-ajo-cuc-automation-PSDK/cjm-runtime/src/test/resources/com/adobe/platform/testing/e2e/{suite_name}\"\n",
    "    \n",
    "    try:\n",
    "        matcher = FeatureStepMatcher(feature_path)\n",
    "        matcher.extract_steps_from_features()\n",
    "        \n",
    "        java_path = \"/Users/ritusaini/Documents/acp-e2e-testing-ajo-cuc-automation-PSDK/cjm-runtime/src/main/java/com/adobe/platform/testing/e2e\"\n",
    "        for root, _, files in os.walk(java_path):\n",
    "            for file in files:\n",
    "                \n",
    "                if file.endswith('.java'):\n",
    "                    print(file)\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as java_file:\n",
    "                        content = java_file.read()\n",
    "                        matcher.parse_step_definitions(content, file_path)\n",
    "        matches = matcher.match_steps_with_implementations()\n",
    "        output_path = f\"step_definitions_{suite_name}.json\"\n",
    "        \n",
    "        \n",
    "        print(\"\\nFeature Step Implementations:\\n\")\n",
    "        for step, details in matches.items():\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Step: {step}\")\n",
    "            print(f\"Feature File: {details['feature_file']}\")\n",
    "            print(f\"Type: {details['type']}\")\n",
    "            \n",
    "            if details['implementation']:\n",
    "                \n",
    "                impl = details['implementation']\n",
    "                json_data[step] = {\"annotation\": impl['annotation'], \"code\": impl['method_content']}\n",
    "                print(f\"Similarity Score: {details['similarity_score']:.2f}\")\n",
    "                print(f\"\\nImplementation:\")\n",
    "                print(f\"File: {impl['file_path']}\")\n",
    "                print(f\"Annotation: {impl['annotation']}\")\n",
    "                print(f\"Method: {impl['method_name']}\")\n",
    "                print(\"\\nCode:\")\n",
    "                print(impl['method_content'])\n",
    "            else:\n",
    "                print(\"\\nNo matching implementation found\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(json_data, json_file, indent=4)\n",
    "        print(f\"Step definitions saved to {output_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_path = f\"/Users/ritusaini/Documents/acp-e2e-testing-ajo-cuc-automation-PSDK/cjm-runtime/src/test/resources/com/adobe/platform/testing/e2e\"\n",
    "\n",
    "    for suite_name in os.listdir(feature_path):\n",
    "        if os.path.isdir(os.path.join(feature_path, suite_name)):\n",
    "            json_data = {}\n",
    "            main(suite_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario to steps mapping\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureParser:\n",
    "    def __init__(self, feature_files_path: str):\n",
    "        self.feature_files_path = Path(feature_files_path)\n",
    "        self.scenarios: Dict[str, Dict] = {}\n",
    "        \n",
    "    def parse_feature_files(self) -> None:\n",
    "        \"\"\"Parse all feature files in the specified directory.\"\"\"\n",
    "        feature_files = list(self.feature_files_path.glob(\"*.feature\"))\n",
    "        \n",
    "        if not feature_files:\n",
    "            logger.warning(f\"No feature files found in {self.feature_files_path}\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Found {len(feature_files)} feature files\")\n",
    "        \n",
    "        for file_path in feature_files:\n",
    "            try:\n",
    "                self._parse_single_feature(file_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing feature file {file_path}: {str(e)}\")\n",
    "                \n",
    "    def _parse_single_feature(self, file_path: Path) -> None:\n",
    "        \"\"\"Parse a single feature file and extract scenarios with their steps.\"\"\"\n",
    "        current_scenario = None\n",
    "        current_steps = []\n",
    "        tags = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            logger.info(f\"Processing feature file: {file_path.name}\")\n",
    "            lines = file.readlines()\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                \n",
    "                if line.startswith('@'):\n",
    "                    tags = line.strip()\n",
    "                    continue\n",
    "                \n",
    "                # Handle Scenario or Scenario Outline\n",
    "                if line.startswith(('Scenario:', 'Scenario Outline:')):\n",
    "                    # Save previous scenario if exists\n",
    "                    if current_scenario and current_steps:\n",
    "                        self._save_scenario(current_scenario, current_steps)\n",
    "                    \n",
    "                    # Start new scenario\n",
    "                    current_scenario = line.split(':', 1)[1].strip()\n",
    "                    current_steps = []\n",
    "                    continue\n",
    "                \n",
    "                # Collect steps for current scenario\n",
    "                if current_scenario and line and not line.startswith('@') and not line.startswith('Feature:'):\n",
    "                    if line.startswith(('Given ', 'When ', 'Then ', 'And ', 'But ')):\n",
    "                        current_steps.append(line)\n",
    "                    elif '|' in line:  # Handle data tables\n",
    "                        current_steps.append(line)\n",
    "            \n",
    "            # Save the last scenario\n",
    "            if current_scenario and current_steps:\n",
    "                self._save_scenario(current_scenario, current_steps)\n",
    "    \n",
    "    def _save_scenario(self, scenario_name: str, steps: list) -> None:\n",
    "        \"\"\"Format and save a scenario with its steps.\"\"\"\n",
    "        # Format steps as a code block with proper indentation\n",
    "        steps_code = '\\n'.join(steps)\n",
    "        \n",
    "        self.scenarios[scenario_name] = {\n",
    "            \"code\": steps_code\n",
    "        }\n",
    "    \n",
    "    def save_to_json(self, output_path: str) -> None:\n",
    "        \"\"\"Save the parsed scenarios and steps to a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(self.scenarios, json_file, indent=4, ensure_ascii=False)\n",
    "            logger.info(f\"Scenarios saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving JSON file: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # Update this path to match your feature files location\n",
    "    feature_path = \"/Users/ritusaini/Documents/acp-e2e-testing-ajo-cuc-automation-PSDK/cjm-runtime/src/test/resources/com/adobe/platform/testing/e2e\"\n",
    "    \n",
    "    for suite_name in os.listdir(feature_path):\n",
    "        suite_path = os.path.join(feature_path, suite_name)\n",
    "        if os.path.isdir(suite_path):\n",
    "            try:\n",
    "                parser = FeatureParser(suite_path)\n",
    "                parser.parse_feature_files()\n",
    "                output_path = f\"scenarios_{suite_name}.json\"\n",
    "                parser.save_to_json(output_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing suite {suite_name}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL Data preparation\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "input_folder_path = '/Users/ritusaini/Documents/OpenAI'\n",
    "training_file_path = 'training_data.jsonl'\n",
    "validation_file_path = 'validation_data.jsonl'\n",
    "\n",
    "validation_split = 0.2\n",
    "all_entries = []\n",
    "\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        input_file_path = os.path.join(input_folder_path, file_name)\n",
    "        \n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                for step, details in data.items():\n",
    "                    system_message = \"You are an AI assistant that provides accurate and concise responses.\"\n",
    "                    user_message = step.strip()\n",
    "                    assistant_message = details.get(\"code\", \"\").strip()\n",
    "                    \n",
    "                    jsonl_entry = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": system_message},\n",
    "                            {\"role\": \"user\", \"content\": user_message},\n",
    "                            {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "                        ]\n",
    "                    }\n",
    "                    all_entries.append(jsonl_entry)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_name}: {e}\")\n",
    "\n",
    "random.shuffle(all_entries)\n",
    "split_index = int(len(all_entries) * (1 - validation_split))\n",
    "training_data = all_entries[:split_index]\n",
    "validation_data = all_entries[split_index:]\n",
    "\n",
    "with open(training_file_path, 'w', encoding='utf-8') as train_file:\n",
    "    for entry in training_data:\n",
    "        train_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "with open(validation_file_path, 'w', encoding='utf-8') as val_file:\n",
    "    for entry in validation_data:\n",
    "        val_file.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps Text File\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_folder_path = '/Users/ritusaini/Documents/OpenAI'\n",
    "output_file_path = 'all_steps.txt'\n",
    "\n",
    "all_steps = []\n",
    "\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.startswith('step_definitions') & file_name.endswith('.json'):\n",
    "        input_file_path = os.path.join(input_folder_path, file_name)\n",
    "        \n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                all_steps.extend(data.keys())\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {file_name}: {e}\")\n",
    "\n",
    "all_steps = list(set(all_steps))\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    for step in all_steps:\n",
    "        outfile.write(step + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario Text File\n",
    "\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def format_scenarios_to_text(input_folder_path: str, output_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Read all scenario JSON files and format them into a text file.\n",
    "    Each scenario name will be followed by its steps on consecutive lines.\n",
    "    \"\"\"\n",
    "    input_folder = Path(input_folder_path)\n",
    "    \n",
    "    scenarios_data = {}\n",
    "    \n",
    "    for file_path in input_folder.glob('scenarios_*.json'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                for scenario_name, details in data.items():\n",
    "                    steps = details['code'].split('\\n')\n",
    "                    scenarios_data[scenario_name] = steps\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON in file {file_path.name}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing file {file_path.name}: {e}\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            for scenario_name, steps in scenarios_data.items():\n",
    "                outfile.write(f\"## Scenario: {scenario_name}\\n\")\n",
    "                \n",
    "                for step in steps:\n",
    "                    if step.strip():\n",
    "                        outfile.write(f\"    {step.strip()}\\n\")\n",
    "                \n",
    "                outfile.write(\"\\n\")\n",
    "                        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to output file: {e}\")\n",
    "\n",
    "def main():\n",
    "    input_folder_path = '/Users/ritusaini/Documents/OpenAI'\n",
    "    output_file_path = 'all_scenarios.txt'\n",
    "    \n",
    "    format_scenarios_to_text(input_folder_path, output_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
